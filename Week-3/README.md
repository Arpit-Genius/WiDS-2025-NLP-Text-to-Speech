# WEEK 3 - Transformers + Modern NLP

## CONCEPTS
### 2017 Transformer architecture (Q/K/V, self-attention)
What problem did Transformer solve?  
### BERT vs GPT vs encoderâ€“decoder models
### Tokenizers (WordPiece, BPE, SentencePiece)

## CODING TASKS
### Fine-Tunned BERT for Text Classification 
Pretrained BERT used(bert-base-uncased) - "tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")"  
Added classification head  
Fine-tuned on the dataset  
Colab Notebook - https://colab.research.google.com/drive/106yLpRkm8nk9X-EetUypjZnSFlJeStsE?usp=sharing  
### Trained a custom SentencePiece Tokenizer
Colab Notebook Link - https://colab.research.google.com/drive/1k7riptZ6YiXjVmsnnRK5AVDDlXWEd91-?usp=sharing  

## MINI-PROJECT
### TEXT SUMMARIZER
Used nltk  
Model: facebook/bart-large-cnn   
Colab Notebbok - https://colab.research.google.com/drive/1zPidqnG--OgPX3KPM99IfYR376g65yuY?usp=sharing




